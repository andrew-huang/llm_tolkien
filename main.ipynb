{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ethan/mambaforge/envs/llm_lotr/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda111.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 111\n",
      "CUDA SETUP: Loading binary /home/ethan/mambaforge/envs/llm_lotr/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cuda111.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethan/mambaforge/envs/llm_lotr/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/ethan/mambaforge/envs/llm_lotr did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/ethan/mambaforge/envs/llm_lotr/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/ethan/mambaforge/etc/xml/catalog file'), PosixPath('file')}\n",
      "  warn(msg)\n",
      "/home/ethan/mambaforge/envs/llm_lotr/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/ethan/mambaforge/envs/llm_lotr/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/ethan/mambaforge/envs/llm_lotr/lib/python3.11/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "INFO:torch.distributed.nn.jit.instantiator:Created a temporary directory at /tmp/tmp1xd7842y\n",
      "INFO:torch.distributed.nn.jit.instantiator:Writing /tmp/tmp1xd7842y/_remote_module_non_scriptable.py\n"
     ]
    }
   ],
   "source": [
    "from llm import config as config_llm\n",
    "from llm.extract_text import extract\n",
    "from llm.prepare_dataset import prepare_dataset\n",
    "from llm.training import LLMTolkien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm.extract_text:Start extracting pages from https://gosafir.com/mag/wp-content/uploads/2019/12/Tolkien-J.-The-lord-of-the-rings-HarperCollins-ebooks-2010.pdf\n",
      "INFO:llm.extract_text:Finished extracting texts from https://gosafir.com/mag/wp-content/uploads/2019/12/Tolkien-J.-The-lord-of-the-rings-HarperCollins-ebooks-2010.pdf\n",
      "INFO:llm.extract_text:Start writing to /home/ethan/llm-tolkien/llm/data/extracted_text.jsonl\n",
      "100%|██████████| 1210/1210 [01:24<00:00, 14.26it/s]\n",
      "1011it [01:25, 11.89it/s]\n",
      "INFO:llm.extract_text:Finished writing to /home/ethan/llm-tolkien/llm/data/extracted_text.jsonl\n"
     ]
    }
   ],
   "source": [
    "extract(\n",
    "    url=config_llm.url, \n",
    "    start_page=config_llm.start_page, \n",
    "    end_page=config_llm.end_page, \n",
    "    header_height=config_llm.header_height, \n",
    "    footer_height=config_llm.footer_height,\n",
    "    extraction_path=config_llm.extraction_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm.prepare_dataset:Start preparing dataset from /home/ethan/llm-tolkien/llm/data/extracted_text.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d059240d1504fc59f3d6e9a53d9ecc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a51794db43493c95f574e7ffd1a308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8abbd70ae2b346f2bd1a3ae5e5fbe6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm.prepare_dataset:The tokenized dataset is composed of 895 elements, each one composed of 2048 tokens.\n",
      "INFO:llm.prepare_dataset:The training dataset is composed of 805 elements, the test dataset is composed of 90 elements.\n",
      "WARNING:datasets.dataset_dict:Pushing split train to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd8f07a6fa441baa9d92dc5b0666ed0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b33aa2f4a5e46a58e7161305734f1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f4516007e843b489114ab2e95bd6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf47a3f9aace4ab2807562287e772827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm.prepare_dataset:Preparing dataset finished.\n"
     ]
    }
   ],
   "source": [
    "prepare_dataset(\n",
    "    dataset_path=config_llm.extraction_path, \n",
    "    min_length=config_llm.min_length,\n",
    "    context_length=config_llm.context_length,\n",
    "    test_size=config_llm.test_size,\n",
    "    shuffle=config_llm.shuffle,\n",
    "    hf_repo=config_llm.hf_repo,\n",
    "    model_name=config_llm.model_name,\n",
    "    hf_book_repo=config_llm.hf_book_repo,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llm.training:Model trainable parameters:\n",
      " trainable params: 4915200 || all params: 3007472640 || trainable%: 0.1634329082375293\n"
     ]
    },
    {
     "ename": "EmptyDatasetError",
     "evalue": "The directory at eturok/llm-tolkien doesn't contain any data files",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDatasetError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m      9\u001b[0m trainer_config \u001b[39m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mper_device_train_batch_size\u001b[39m\u001b[39m\"\u001b[39m: config_llm\u001b[39m.\u001b[39mper_device_train_batch_size, \n\u001b[1;32m     11\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mgradient_accumulation_steps\u001b[39m\u001b[39m\"\u001b[39m: config_llm\u001b[39m.\u001b[39mgradient_accumulation_steps,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mpush_to_hub\u001b[39m\u001b[39m\"\u001b[39m: config_llm\u001b[39m.\u001b[39mpush_to_hub\n\u001b[1;32m     23\u001b[0m }\n\u001b[1;32m     25\u001b[0m model \u001b[39m=\u001b[39m LLMTolkien(config_llm\u001b[39m.\u001b[39mmodel_name)\n\u001b[0;32m---> 26\u001b[0m model\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m     27\u001b[0m     hf_repo\u001b[39m=\u001b[39;49mconfig_llm\u001b[39m.\u001b[39;49mhf_repo,\n\u001b[1;32m     28\u001b[0m     lora_config\u001b[39m=\u001b[39;49mlora_config,\n\u001b[1;32m     29\u001b[0m     trainer_config\u001b[39m=\u001b[39;49mtrainer_config,\n\u001b[1;32m     30\u001b[0m     mlm\u001b[39m=\u001b[39;49mconfig_llm\u001b[39m.\u001b[39;49mmlm\n\u001b[1;32m     31\u001b[0m )\n",
      "File \u001b[0;32m~/llm-tolkien/llm/training.py:37\u001b[0m, in \u001b[0;36mLLMTolkien.train\u001b[0;34m(self, hf_repo, lora_config, trainer_config, mlm)\u001b[0m\n\u001b[1;32m     35\u001b[0m model \u001b[39m=\u001b[39m get_peft_model(model, LoraConfig(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mlora_config))\n\u001b[1;32m     36\u001b[0m LOGGER\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel trainable parameters:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mprint_trainable_parameters(model)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(hf_repo)\n\u001b[1;32m     38\u001b[0m LOGGER\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain dataset downloaded:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mdataset[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m LOGGER\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of tokens for the training: \u001b[39m\u001b[39m{\u001b[39;00mdataset[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mnum_rows\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(dataset[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/llm_lotr/lib/python3.11/site-packages/datasets/load.py:1785\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1780\u001b[0m verification_mode \u001b[39m=\u001b[39m VerificationMode(\n\u001b[1;32m   1781\u001b[0m     (verification_mode \u001b[39mor\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mBASIC_CHECKS) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m save_infos \u001b[39melse\u001b[39;00m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS\n\u001b[1;32m   1782\u001b[0m )\n\u001b[1;32m   1784\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[0;32m-> 1785\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[1;32m   1786\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m   1787\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1788\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1789\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1790\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1791\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[1;32m   1792\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1793\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1794\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1795\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1796\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   1797\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[1;32m   1798\u001b[0m )\n\u001b[1;32m   1800\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[1;32m   1801\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[0;32m~/mambaforge/envs/llm_lotr/lib/python3.11/site-packages/datasets/load.py:1514\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1512\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m   1513\u001b[0m     download_config\u001b[39m.\u001b[39muse_auth_token \u001b[39m=\u001b[39m use_auth_token\n\u001b[0;32m-> 1514\u001b[0m dataset_module \u001b[39m=\u001b[39m dataset_module_factory(\n\u001b[1;32m   1515\u001b[0m     path,\n\u001b[1;32m   1516\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1517\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1518\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1519\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[1;32m   1520\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[1;32m   1521\u001b[0m )\n\u001b[1;32m   1523\u001b[0m \u001b[39m# Get dataset builder class from the processing script\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m builder_cls \u001b[39m=\u001b[39m import_main_class(dataset_module\u001b[39m.\u001b[39mmodule_path)\n",
      "File \u001b[0;32m~/mambaforge/envs/llm_lotr/lib/python3.11/site-packages/datasets/load.py:1165\u001b[0m, in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[39mreturn\u001b[39;00m LocalDatasetModuleFactoryWithScript(\n\u001b[1;32m   1160\u001b[0m         combined_path, download_mode\u001b[39m=\u001b[39mdownload_mode, dynamic_modules_path\u001b[39m=\u001b[39mdynamic_modules_path\n\u001b[1;32m   1161\u001b[0m     )\u001b[39m.\u001b[39mget_module()\n\u001b[1;32m   1162\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(path):\n\u001b[1;32m   1163\u001b[0m     \u001b[39mreturn\u001b[39;00m LocalDatasetModuleFactoryWithoutScript(\n\u001b[1;32m   1164\u001b[0m         path, data_dir\u001b[39m=\u001b[39;49mdata_dir, data_files\u001b[39m=\u001b[39;49mdata_files, download_mode\u001b[39m=\u001b[39;49mdownload_mode\n\u001b[0;32m-> 1165\u001b[0m     )\u001b[39m.\u001b[39;49mget_module()\n\u001b[1;32m   1166\u001b[0m \u001b[39m# Try remotely\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m \u001b[39melif\u001b[39;00m is_relative_path(path) \u001b[39mand\u001b[39;00m path\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/mambaforge/envs/llm_lotr/lib/python3.11/site-packages/datasets/load.py:640\u001b[0m, in \u001b[0;36mLocalDatasetModuleFactoryWithoutScript.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_module\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DatasetModule:\n\u001b[1;32m    638\u001b[0m     base_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_dir \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath\n\u001b[1;32m    639\u001b[0m     patterns \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 640\u001b[0m         sanitize_patterns(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_files \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m get_data_patterns_locally(base_path)\n\u001b[1;32m    641\u001b[0m     )\n\u001b[1;32m    642\u001b[0m     data_files \u001b[39m=\u001b[39m DataFilesDict\u001b[39m.\u001b[39mfrom_local_or_remote(\n\u001b[1;32m    643\u001b[0m         patterns,\n\u001b[1;32m    644\u001b[0m         base_path\u001b[39m=\u001b[39mbase_path,\n\u001b[1;32m    645\u001b[0m         allowed_extensions\u001b[39m=\u001b[39mALL_ALLOWED_EXTENSIONS,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[1;32m    647\u001b[0m     split_modules \u001b[39m=\u001b[39m {\n\u001b[1;32m    648\u001b[0m         split: infer_module_for_data_files(data_files_list) \u001b[39mfor\u001b[39;00m split, data_files_list \u001b[39min\u001b[39;00m data_files\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    649\u001b[0m     }\n",
      "File \u001b[0;32m~/mambaforge/envs/llm_lotr/lib/python3.11/site-packages/datasets/data_files.py:447\u001b[0m, in \u001b[0;36mget_data_patterns_locally\u001b[0;34m(base_path)\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[39mreturn\u001b[39;00m _get_data_files_patterns(resolver)\n\u001b[1;32m    446\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m--> 447\u001b[0m     \u001b[39mraise\u001b[39;00m EmptyDatasetError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe directory at \u001b[39m\u001b[39m{\u001b[39;00mbase_path\u001b[39m}\u001b[39;00m\u001b[39m doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt contain any data files\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mEmptyDatasetError\u001b[0m: The directory at eturok/llm-tolkien doesn't contain any data files"
     ]
    }
   ],
   "source": [
    "lora_config = {\n",
    "    \"r\": config_llm.lora_r,\n",
    "    \"lora_alpha\": config_llm.lora_alpha,\n",
    "    \"lora_dropout\": config_llm.lora_dropout, \n",
    "    'bias': config_llm.lora_bias,\n",
    "    \"task_type\": config_llm.lora_task_type,\n",
    "}\n",
    "\n",
    "trainer_config = {\n",
    "    \"per_device_train_batch_size\": config_llm.per_device_train_batch_size, \n",
    "    \"gradient_accumulation_steps\": config_llm.gradient_accumulation_steps,\n",
    "    \"warmup_steps\": config_llm.warmup_steps,\n",
    "    \"weight_decay\": config_llm.weight_decay,\n",
    "    \"num_train_epochs\": config_llm.num_train_epochs,\n",
    "    \"learning_rate\": config_llm.learning_rate, \n",
    "    \"fp16\": config_llm.fp16,\n",
    "    \"logging_steps\": config_llm.logging_steps, \n",
    "    \"output_dir\": config_llm.hf_repo,\n",
    "    \"overwrite_output_dir\": config_llm.overwrite_output_dir,\n",
    "    \"evaluation_strategy\": config_llm.evaluation_strategy,\n",
    "    \"save_strategy\": config_llm.save_strategy,\n",
    "    \"push_to_hub\": config_llm.push_to_hub\n",
    "}\n",
    "\n",
    "model = LLMTolkien(config_llm.model_name)\n",
    "model.train(\n",
    "    hf_repo=config_llm.hf_repo,\n",
    "    lora_config=lora_config,\n",
    "    trainer_config=trainer_config,\n",
    "    mlm=config_llm.mlm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_lotr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
